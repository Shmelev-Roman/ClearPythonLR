{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=14, n_informative=10, noise=15, random_state=42)\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Линейная регрессия\n",
    "class MyLineReg():\n",
    "    # Инициализация класса\n",
    "    def __init__(self, weights=None, \n",
    "                 n_iter: int=100, \n",
    "                 learning_rate: int=0.1, \n",
    "                 metric: str=None,\n",
    "                 reg: str=None,\n",
    "                 l1_coef: float=0,\n",
    "                 l2_coef: float=0,\n",
    "                 sgd_sample: float=None,\n",
    "                 random_state: int=42,) -> None:\n",
    "        self.weights = weights\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.metric = metric\n",
    "        self.reg = reg\n",
    "        self.regflag = self.reg in ['l1', 'l2', 'elasticnet']\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "        self.best_score = 0\n",
    "        \n",
    "        \n",
    "    # Вывод информации при передаче в print()\n",
    "    def __str__(self) -> str:\n",
    "        return f\"MyLineReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\"\n",
    "\n",
    "    # Алгоритм тренировки модели\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, verbose: int=0) -> None:\n",
    "        random.seed(self.random_state)\n",
    "        # Дополняем матрицу фичей столбцом единиц слева для w0\n",
    "        X = np.concatenate((np.ones(len(X))[:, np.newaxis], X), axis=1)\n",
    "        y = np.array(y)\n",
    "        self.weights = np.ones(X.shape[1])\n",
    "\n",
    "        # процесс обучения\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            # Формируем мини-батч и делаем предсказания\n",
    "            if not self.sgd_sample or self.sgd_sample <= 0:\n",
    "                sample_rows_idx = np.arange(0, len(X))\n",
    "            elif self.sgd_sample > 1 and isinstance(self.sgd_sample, int):\n",
    "                sample_rows_idx = random.sample(range(X.shape[0]), self.sgd_sample) \n",
    "            else:\n",
    "                sample_rows_idx = random.sample(range(X.shape[0]), int(self.sgd_sample * len(X)))\n",
    "            mini_X = X[sample_rows_idx]\n",
    "            mini_y = y[sample_rows_idx]\n",
    "            pred = np.matmul(mini_X, self.weights.T)        \n",
    "            \n",
    "            # Логи\n",
    "            if verbose and i % verbose == 0:\n",
    "                # Подсчет ошибки\n",
    "                pred_for_metric = np.matmul(X, self.weights.T)\n",
    "                MSE = np.sum(np.power(pred_for_metric - y, 2)) / len(pred_for_metric)\n",
    "                print(f\"Iteration {i}, loss: {MSE}, {self.metric}: {self.__calc_metrics(pred_for_metric, y)}\")\n",
    "            # подсчет градиента\n",
    "            gradient = (2 / len(pred)) * np.matmul((pred - mini_y).T, mini_X)\n",
    "            # обновление весов\n",
    "            lr = 0\n",
    "            if callable(self.learning_rate):\n",
    "                lr = self.learning_rate(i)\n",
    "            else:\n",
    "                lr = self.learning_rate\n",
    "            self.weights = self.weights - lr * \\\n",
    "            (gradient + self.regflag * self.__calc_reg(gradient=True))\n",
    "        \n",
    "        pred = np.matmul(X, self.weights.T)\n",
    "        self.best_score = self.__calc_metrics(pred, y)\n",
    "    \n",
    "    # Получение весов модели\n",
    "    def get_coef(self) -> np.ndarray:\n",
    "        return self.weights[1:]\n",
    "    \n",
    "    # Предсказания\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.concatenate((np.ones(len(X))[:, np.newaxis], X), axis=1)\n",
    "        return np.matmul(X, self.weights.T)\n",
    "    \n",
    "    # Метрики\n",
    "    def __calc_metrics(self, pred: np.ndarray, y: np.ndarray) -> np.float64:\n",
    "        if self.metric not in ['mae', 'mse', 'rmse', 'mape', 'r2']:\n",
    "            return None\n",
    "        \n",
    "        elif self.metric == 'mae':\n",
    "            return np.sum(np.abs(pred - y)) / len(pred)\n",
    "        elif self.metric == 'mse':\n",
    "            return np.sum(np.power(pred - y, 2)) / len(pred)\n",
    "        elif self.metric == 'rmse':\n",
    "            return np.sqrt(np.sum(np.power(pred - y, 2)) / len(pred))\n",
    "        elif self.metric == 'r2':\n",
    "            return 1 - (np.sum(np.power(y - pred, 2)) / np.sum(np.power(y - y.mean(), 2)))\n",
    "        elif self.metric == 'mape':\n",
    "            return np.sum(np.abs((y - pred) / y)) / len(pred) * 100\n",
    "\n",
    "    # лучший результат модели\n",
    "    def get_best_score(self) -> np.float64:\n",
    "        return self.best_score\n",
    "    \n",
    "    # подсчет регуляризации\n",
    "    def __calc_reg(self, gradient: bool=False) -> np.ndarray:        \n",
    "        l1 = np.sum(np.abs(self.weights))\n",
    "        l1dif = np.sign(self.weights) * self.l1_coef\n",
    "        l2 = np.sum(np.power(self.weights, 2)) * self.l2_coef\n",
    "        l2dif = 2 * self.weights * self.l2_coef\n",
    "        if not gradient:\n",
    "            if self.reg == 'l1':\n",
    "                return l1\n",
    "            elif self.reg == 'l2':\n",
    "                return l2\n",
    "            return l1 + l2\n",
    "        if self.reg == 'l1':\n",
    "            return l1dif\n",
    "        elif self.reg == 'l2':\n",
    "            return l2dif\n",
    "        return l1dif + l2dif\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss: 4577.424689371495, mae: 53.852504812448274\n",
      "Iteration 10, loss: 644.8204293770057, mae: 20.129234243885367\n",
      "Iteration 15, loss: 275.18165467946324, mae: 13.324546732719392\n",
      "Iteration 20, loss: 230.72956743654993, mae: 12.17361663404447\n",
      "Iteration 25, loss: 228.54338146051754, mae: 12.083618424347463\n",
      "Iteration 30, loss: 226.3324916046762, mae: 12.046104339332379\n",
      "Iteration 35, loss: 225.41862681182985, mae: 12.079855223693361\n",
      "Iteration 40, loss: 225.51771346164134, mae: 12.056696339206987\n",
      "Iteration 45, loss: 229.74278066621216, mae: 12.117626896790489\n",
      "Iteration 50, loss: 226.70960486645845, mae: 12.07443502691104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.377720712645836"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLineReg(n_iter=50, learning_rate=0.1, metric='mae', sgd_sample=0.1, random_state=42)\n",
    "linear.fit(X, y, verbose=5)\n",
    "linear.get_coef().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
